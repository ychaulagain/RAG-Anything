{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQinUPQ1iKiRKDT+T8RoCx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychaulagain/RAG-Anything/blob/main/notebooks/02_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 – Autograd (Automatic Differentiation)\n",
        "\n",
        "## Goals\n",
        "By the end of this notebook, you will be able to:\n",
        "- Explain what **autograd** does in PyTorch\n",
        "- Use `requires_grad` to track gradients\n",
        "- Understand what `.backward()` computes\n",
        "- Inspect gradients using `.grad`\n",
        "- Know why you need to **zero gradients**\n",
        "- Use `torch.no_grad()` and `detach()` correctly\n"
      ],
      "metadata": {
        "id": "08CigE69tGsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpRkZjhItFES",
        "outputId": "9b987c72-2181-437a-ff1a-5c5156a9c429"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is autograd?\n",
        "**Autograd** is PyTorch's automatic differentiation engine.\n",
        "\n",
        "When you perform operations on tensors with `requires_grad=True`, PyTorch:\n",
        "- builds a **computation graph** (records operations)\n",
        "- can compute derivatives (gradients) using **backpropagation**\n",
        "\n",
        "In deep learning:\n",
        "- weights are tensors\n",
        "- loss is a scalar tensor\n",
        "- gradients tell us how to update weights to reduce loss"
      ],
      "metadata": {
        "id": "nD09e-ANtaCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2 + 3*x + 1\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"y.requires_grad:\", y.requires_grad)\n",
        "print(\"y.grad_fn\", y.grad_fn) # shows the function tat created y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKzprUSltUqR",
        "outputId": "5a0dd3bc-4847-4874-c3ab-e22a76552975"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor(2., requires_grad=True)\n",
            "y: tensor(11., grad_fn=<AddBackward0>)\n",
            "y.requires_grad: True\n",
            "y.grad_fn <AddBackward0 object at 0x7d3ba928b820>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. `.backward()` and gradients\n",
        "If `y` is a scalar,calling:\n",
        "```python\n",
        "y.backward()\n",
        "Computes the gradient dy/dx and stores it in `x.grad`"
      ],
      "metadata": {
        "id": "1dD-voOTCQcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Compute gradient for scalar (Code)\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2 + 3*x + 1  # derivative: dy/dx = 2x + 3\n",
        "\n",
        "y.backward()\n",
        "print(\"y:\", y.item())\n",
        "print(\"x.grad:\", x.grad.item())  # should be 2*2 + 3 = 7\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk8eGeN3DH2w",
        "outputId": "66a0a76c-3452-498d-fc1c-fc62c5d36e64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: 11.0\n",
            "x.grad: 7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Gradients accumulate\n",
        "If you call `.backward()` multiple times, gradientsts are **added** to `.grad`. This is why training loops usually do `optimizer.zero_grad()` each iteration."
      ],
      "metadata": {
        "id": "N_K1ye55DhIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "y.backward()\n",
        "print(\"After first backward, x.grad:\", x.grad.item()) # 2x = 4\n",
        "\n",
        "y = x**2\n",
        "y.backward()\n",
        "print(\"After second backward, x.grad:\", x.grad.item()) # accumulates -> 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUucPIWHD9lM",
        "outputId": "3d898482-64a0-4f15-80f4-f0eeebd69ff5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After first backward, x.grad: 4.0\n",
            "After second backward, x.grad: 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "print(\"After zero_, x.grad\", x.grad.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmrM41X4EhWW",
        "outputId": "9d6f7c6a-f071-46b1-e280-b1ca0469470f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After zero_, x.grad 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Autograd with vectors (and why loss is usually a scalar)\n",
        "If the output is **not scalar**, PyTorch needs to know how to combine values to compute gradients.\n",
        "\n",
        "Most training uses a **scalar loss** (e.g., mean loss), so `.backward()` works naturally."
      ],
      "metadata": {
        "id": "Y_A6SrQWE5a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "# Make it scalar using sum or (mean) so backward() is defined\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"loss:\", loss)\n",
        "print(\"x.grad:\", x.grad) # derivative of sum(x^2) - 2x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVWpydZFh0z",
        "outputId": "82076d48-4e67-439a-c960-13cd18678219"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([1., 2., 3.], requires_grad=True)\n",
            "y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "loss: tensor(14., grad_fn=<SumBackward0>)\n",
            "x.grad: tensor([2., 4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Vector example of scalar reduction\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x**2 # vector output\n",
        "\n",
        "# Make it scalar using sum (or mean) so backward() is defined\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"loss:\", loss)\n",
        "print(\"x.grad:\", x.grad) # derivative of sum(x^2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2tgt7KwG3yM",
        "outputId": "7c1002f3-0422-4738-c58c-a90ed1ea070a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([1., 2., 3.], requires_grad=True)\n",
            "y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "loss: tensor(14., grad_fn=<SumBackward0>)\n",
            "x.grad: tensor([2., 4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward on a non-scalar tensor\n"
      ],
      "metadata": {
        "id": "pJPQEDQxQezF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x**2 # vector\n",
        "\n",
        "# Provide grad_output (same shape as y)\n",
        "grad_output = torch.tensor([1.0, 1.0, 1.0])\n",
        "y.backward(gradient=grad_output)\n",
        "\n",
        "\n",
        "print(\"y:\", y)\n",
        "print(\"x.grad:\", x.grad) # still 2x because grad_output is ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGu-7ICoRSMq",
        "outputId": "d472e6eb-6fbf-4709-facb-ea2722ca5437"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "x.grad: tensor([2., 4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. `detach()` vs `torch.no_grad()`\n",
        "\n",
        "### `detach()`\n",
        "Creates a new tensor that shares the same data but **stops tracking gradients**.\n",
        "\n",
        "### `torch.no_grad()`\n",
        "A context manager to temorarily disable gradient tracking.\n",
        "Used during **inference/evaluation** to save memory and compute"
      ],
      "metadata": {
        "id": "pQGBNQLQUIfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## detach example\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "y_detached = y.detach()\n",
        "print(\"y.requires_grad:\", y.requires_grad)\n",
        "print(\"y_detached.requires_grad:\", y_detached.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dftTGUJVGDN",
        "outputId": "af73ff47-34af-44bd-d5a3-877cfe538114"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y.requires_grad: True\n",
            "y_detached.requires_grad: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## no_grad example\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2\n",
        "\n",
        "with torch.no_grad():\n",
        "  y = x**2 + 5\n",
        "print(\"y:\", y)\n",
        "print(\"y.requires_grad:\", y.requires_grad) # False, because no_grad block\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ngj6ym7VxwK",
        "outputId": "cfc3e7d9-f9d8-40e0-b1b4-de53766bd87c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor(9.)\n",
            "y.requires_grad: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Turning gradinet tracking on/off\n",
        "\n",
        "You can enable grad tracking on an existing tensor using:\n",
        "```python\n",
        "x.requires_grad_()"
      ],
      "metadata": {
        "id": "2FgARiiKYYdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### requires_grad_ example\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(\"Before:\", x.requires_grad)\n",
        "\n",
        "x.requires_grad_()\n",
        "print(\"After:\", x.requires_grad)\n",
        "\n",
        "y = (x ** 2).mean()\n",
        "y.backward()\n",
        "print(\"x.grad:\", x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWNo7G77Y491",
        "outputId": "e73034dc-3bc6-47ba-811a-9aa2a154f196"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: False\n",
            "After: True\n",
            "x.grad: tensor([0.6667, 1.3333, 2.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Mini example: gradinet descent intuition\n",
        "We'll minimize\n",
        "\\[\n",
        "  f(w) = (w - 3)^2\n",
        "\\]\n",
        "\n",
        "The minimum is at `w = 3`\n",
        "We'll use autograd to compute gradients and update `w`."
      ],
      "metadata": {
        "id": "IxKu-y0bdIeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(0.0, requires_grad=True)\n",
        "lr = 0.01\n",
        "\n",
        "for step in range(10):\n",
        "  loss = (w - 3) ** 2\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad # gradient descent step\n",
        "\n",
        "  w.grad.zero_() # reset gradients\n",
        "  print(f\"step={step:02d} w={w.item():.4f} loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8cT-4xUdpML",
        "outputId": "3c6e20f1-2b78-4951-8e80-82e34dc7c962"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step=00 w=0.0600 loss=9.0000\n",
            "step=01 w=0.1188 loss=8.6436\n",
            "step=02 w=0.1764 loss=8.3013\n",
            "step=03 w=0.2329 loss=7.9726\n",
            "step=04 w=0.2882 loss=7.6569\n",
            "step=05 w=0.3425 loss=7.3537\n",
            "step=06 w=0.3956 loss=7.0625\n",
            "step=07 w=0.4477 loss=6.7828\n",
            "step=08 w=0.4988 loss=6.5142\n",
            "step=09 w=0.5488 loss=6.2562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common mistakes (and how to avoid them)\n",
        "\n",
        "1. **Forgetting `requires_grad=True`**\n",
        "   - Gradients will be `None`\n",
        "\n",
        "2. **Calling `.backward()` repeatedly without resetting grads**\n",
        "   - Gradients accumulate and updates explode\n",
        "\n",
        "3. **Trying `.backward()` on a non-scalar tensor**\n",
        "   - Use `.sum()` / `.mean()` or pass `gradient=...`\n",
        "\n",
        "4. **Using gradients during inference**\n",
        "   - Wrap inference with `torch.no_grad()` to save memory and speed up\n"
      ],
      "metadata": {
        "id": "3qMIJCsqlqjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You learned:\n",
        "- What autograd is and why it matters\n",
        "- How `requires_grad` controls tracking\n",
        "- How `.backward()` computes gradients and stores them in `.grad`\n",
        "- Why gradients accumulate and how to reset them\n",
        "- How to handle vector outputs (reduce to scalar or pass gradient)\n",
        "- The difference between `detach()` and `torch.no_grad()`\n",
        "\n",
        "Next: **03 – `nn.Module` and building neural networks**\n"
      ],
      "metadata": {
        "id": "luqnmT5fv7L8"
      }
    }
  ]
}